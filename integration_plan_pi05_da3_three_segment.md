# PI0.5 三段注意力（视觉+文本 / 几何 / 动作）接入规划

# PI0.5 几何注入（双 cross 方案）

目标：不改变现有 VLM/动作头数配置，保持视觉前缀/动作自注意力不变，动作通过两路 cross-attn 分别读取视觉+文本 KV 和几何 KV，几何不走动作 FFN/门控。

## 方案：动作自注意力 + 前缀 cross + 几何 cross
- 前缀（视觉+文本）：保持原自注意力/FFN。
- 动作：保持原自注意力/FFN。
- 几何：适配器输出后，独立几何 k/v 投影（自带可学习缩放 α），仅提供 KV，不参与动作 FFN。
- 融合：动作 query 先跨注意力读取前缀 KV，再跨注意力读取几何 KV（或并行计算后加和），再进动作 FFN/门控。
- cache：几何 KV 可固定不变（可复用），保持原有 cache 逻辑。

## 当前状态
- 代码已实现动作自注意力 + 几何 cross（geom_k/v_proj+alpha），几何不走动作 FFN。
- 前缀保持原自注意力；动作注意力中仍包含前缀 KV，几何通过额外 cross 注入。
- 几何 cross 目前未应用 pad/att mask，默认全有效；训练/评估管线尚未自动生成几何 token。
- `tools/test_three_segment_pi05.py` 前向已验证通过（单张图，geom_tokens -> actions）。

## 下一步规划（按优先级）
1) **接入训练/评估管线**：复用 DA3→GeomAdapter 生成 `geom_tokens`，在 forward/sample 时传入，先跑通训练（几何全有效，模型接口已支持 `geom_tokens`）。
2) **补几何 mask/位置**：仿前缀生成几何 pad/att mask，cross-attn 时应用，必要时补 RoPE/pos 处理。
3) **小规模验证**：小网格、小 batch 跑一次训练+推理，检查 loss/形状/显存，适当调节 geom_alpha。
4) **优化（可选）**：如需进一步控显存或开 cache，再评估几何 KV 的缓存策略。

## 注意
- 不再强拼多段 KV，头数不需要强行一致；几何与前缀互不干扰，只通过动作查询聚合。
- 显存相对三段自注意力更可控；几何网格仍建议从小网格起步。
